Environment Setup:
    tensorflow 2.2.0
      pip install tensorflow
      pip install tensorflow-addons

    spaCy (macOS)
      pip install -U spacy
      python3 -m spacy download en_core_web_sm

    DyNet
      pip install dynet

    pathlib
      pip install pathlib

============================= NER =============================

TRAINING
  Dataset:
    1. SFM starter dataset: https://github.com/security-force-monitor/nlp_starter_dataset
    2. CONLL2003: https://github.com/guillaumegenthial/tf_ner/tree/master/data/example
    3. A set of known organizations from the starter dataset
    Note: Title and role were collapsed into one class

  Usage:
    1) Prepare data
      $ python process.py
      $ cd SFM_STARTER
      $ python build_vocab.py
      $ python build_glove.py
      $ cd ..

    2) Train model
      $ python train.py

    3) Make predictions
      $ python pred.py

    4) Evaluate model
      $ python eval.py
      $ python eval_class.py

  Files:
    process.py: 1) preprocess dataset by recording info in dicts,
                      which are saved in two pickle files: dataset_labels.pickle, dataset_sentences.pickle
                2) convert SFM starter dataset to a format that can be used by the model,
                      which are in files: {}.words.txt and {}.tags.txt where {} could be train, valid or test.
    pred.py: generates predictions using the trained model
    eval.py: evaluate the predctions made by model, which are generated by running pred.py
    eval_class.py: get precision, recall and f1 score for each class

    Other files are from https://github.com/guillaumegenthial/tf_ner/tree/master/models/chars_conv_lstm_crf_ema
      train.py, interact.py, masked_conv.py, SFM_STARTER/build_vocab.py, SFM_STARTER/build_glove.py

PREDICTING
  Usage:
    $ python ner.py <doc_id>.txt

  File:
    ner.py: get BRAT format prediction for a text file.


============================= RE =============================
jPTDP:
  Before running the following 3 methods, you need to run a dependency parser first, which some methods relies on.
  Usage: Go to the jPTDP directory and run
    $ python fast_parse.py <path_to_txt>.txt
  The output will be put along side with the input text file in a directory whose name is same as the text file.

--- METHOD 1: nearest person:
    Assign the non-person name entities to the nearest person that is behind the name entities.

    Usage:
      1. To extraction relations in a single text file:
        (extracted relations will be appended to the .ann file)
        $ python relation_np.py <doc_id>.txt <doc_id>.ann
      2. To generate annotations for a set of text file under <directory>
        Set "output_dir" in pipeline.sh to <directory> and run:
        $ source pipeline.sh

--- METHOD 2: dependency parsing
    Assign the non-person name entities to the closest person where distance is the length of the dependency path between the name entity and the person
    Constraint: If we only choose from one of the two person that appear immediately on the left and the right side, the results could be improved but the drawbacks are also obvious

    Usage:
      1. To extraction relations in a single text file:
        (extracted relations will be appended to the .ann file)
        $ python relation_dep.py <jPTDP_buffer_path> <doc_id>.txt <doc_id>.ann
      2. To generate annotations for a set of text file under <directory>
        $ source pipeline.sh <directory>

--- METHOD 3: neural networks
    Use dependency path and its distance as features to predict which person in the sentence is the best option
    The best model is saved in "model_86.h5"

    Usage:
      Predictions are made on files in "pred_path" and are written in place, "pred_path" can be set in config.py
      $ python pred.py


/utils:
  - parse_script.py:
      When placed in jPTDP-master folder and run, this script generates .conllu files, one for each line of a set of documents.

  - path.py:
      Functions for getting the paths/path patterns between name entities
      ( An example of path pattern: tuple ('pobj', 'prep', 'dobj', 'prep', 'pobj') )

  - parse.py:
      Functions for reading .conllu files outputted by jPTDP dependency parser and for calculating distances (dependency path length) between name entities

/eval:
  - eval.py:
      Evaluate the precision, recall and F1 score of the outputs from different methods
      Note: The metrics are only including the true positive persons, false negative persons

  - pattern.py
      Count different path patterns

  - ne_only.py
      A script for removing all relations in .ann files and only keep name entities.

  - diff_ann.py
      Compare predicted annotations and ground truth annotations and print relations where the two do not match.

  - table.py
      Output a csv version of the output of diff_ann.py

  - compare.sh
      Show difference between two set of predicted annotations
